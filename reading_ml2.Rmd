---
title: "Reading and video material for ML2 (Tools)"
subtitle: "CEU 2021"
author: "Papp Zoltan"
date: '2021-02-07'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

The course is building on the book Elements of statistical learning (ESLII going forward). Free pdf is available at https://web.stanford.edu/~hastie/Papers/ESLII.pdf.

Only a very small part of the book is actually required for the course. Below you will find all the chapters (page numbers) that are recommended. You can also find references to the chapters in the slides, at the beginning of each topic.

### Tree based methods: 
ESLII 9.2 Tree-Based Methods (page 305-310)

### Bootstrap aggregation: 
ESLII 8.7 Bagging (page 282-288)

### Random forest: 
ESLII 15 Random Forest (page 587-602)

### Boosting: 
ESLII 10.1 Boosting Methods (page 337-341)

### Gradient boosting: 
 - ESLII 10.10.2 Gradient Boosting (page 359-361)

### Support vector machines:
Read ESLII 12.1-12.2 Tree-Based Methods (page 417-422)

### Neural networks: 
  - ESLII 11.3, 11.4, 11.5, 11.6 Neural Networks (page 392-404)
  - 3BLUE1BROWN SERIES: What is backpropagation really doing? https://www.youtube.com/watch?v=Ilg3gGewQ5U
  - 3BLUE1BROWN SERIES: Backpropagation calculus https://www.youtube.com/watch?v=Ilg3gGewQ5U&t=699s
  - Batch normalization: https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c

## Readings that are not required, but are useful if you want to go deeper:
  - Econometric Analysis by William H. Greene: this book provides a mathematical background and is far beyound the scope of our course. The appendix also has a nice summary of Matrix algebra, Probability and distribution theory and other related topics.
  - Introduction to Evolutionary Computing, A.E. Eiben, J.E. Smith





